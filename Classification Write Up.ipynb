{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project was to use classification models to predict the attrition of high performers at IBM. I worked with data provided from IBM for the specific use of understanding features that may influence what High Performers that would be most at risk of leaving the company based of previous data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project originated IBM Watson. Classifying and identifying variables/features that are most useful for understanding what , fi any attributes, may used to identify employee groups most at risk of exiting the company. This is incredible useful as the Talent Market sees one of the largest \"reshuffling\" moments in many years as we accept a new normal with the Endemic/Pandemic three years in. Intervening on behalf of the most high performing talent allows us to access considerable cost savings as well as develop the best talent for the organization moving forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The dataset contains 1,470 Employee observations with 34 features for each, 6 of which are categorical. These features are particularly robust including Employee Demographic information (Age, Gender), Job Role Information (Salary, Performace Ratings, Depatment), and Employee listening system information (Job Satisfaction, Relationship Satisfaction survey data).  All of these features were considered in the analysis in order to understand what attributes might most influence attrition outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering\n",
    "1.Created Binary Integer MApping for Attrition and Overtime variables\n",
    "2.Converted 6 categorical features to binary dummy variables\n",
    "\n",
    "Model Selection\n",
    "\n",
    "Logistic regression, k-nearest neighbors, and logistic regression, and random forest classifiers were all tested using cross validation. Before standard Scaling, random forrest came out on top. Stnadard scaling was necessary as I had many categorical and binary variables as well as variables detailing salary with different scales of measurement. Afterwards, across all 5 cvs, Logistic regression outperformed all other models/ Becasue we had included all features in the analysis, I added a quick lassoo at the end to penalize the model for the number of features and highlight the most impactful models to the target (attrition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slides and Insights were presented to the client as well as suggestions for future work quarterly as the talent market is chaning quickly. Additional ideas were to create focus groups for the identified groups (Sales and Lab Techs) as well as younger employees who may be expecially vulnerable right now. Additionally, groups of employees who traveled fequently or had large amounts of Overtime were also highlighted in the model. With Hybrid becoming more and more seen, we also suggest programming around work place balance as a lever for retention as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
